{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc04eade-5c1a-4be6-a0d5-369689afdc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class RLRecommender:\n",
    "    def __init__(self, n_products, n_states, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.n_products = n_products  # Number of products\n",
    "        self.n_states = n_states  # Number of states (user profiles, for simplicity)\n",
    "        self.learning_rate = learning_rate  # Alpha (Q-learning parameter)\n",
    "        self.discount_factor = discount_factor  # Gamma (Q-learning parameter)\n",
    "        self.epsilon = epsilon  # Exploration rate (epsilon-greedy strategy)\n",
    "        \n",
    "        # Initialize Q-table: rows represent states, columns represent actions (products)\n",
    "        self.q_table = np.zeros((n_states, n_products))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy strategy.\n",
    "        With probability epsilon, choose a random product (explore).\n",
    "        With probability (1 - epsilon), choose the product with the highest Q-value (exploit).\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # Explore: choose a random product\n",
    "            return random.randint(0, self.n_products - 1)\n",
    "        else:\n",
    "            # Exploit: choose the product with the highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning update rule.\n",
    "        \"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        # Q-learning update rule\n",
    "        self.q_table[state, action] = self.q_table[state, action] + self.learning_rate * (\n",
    "            reward + self.discount_factor * self.q_table[next_state, best_next_action] - self.q_table[state, action])\n",
    "    \n",
    "    def get_recommendation(self, state):\n",
    "        \"\"\"\n",
    "        Get the recommended product based on the current state.\n",
    "        \"\"\"\n",
    "        return self.choose_action(state)\n",
    "    \n",
    "    def simulate_interaction(self, state, action):\n",
    "        \"\"\"\n",
    "        Simulate user interaction:\n",
    "        - If the action (recommended product) leads to a \"click\" (reward=1) or \"no click\" (reward=0),\n",
    "        - we update the Q-table accordingly.\n",
    "        \"\"\"\n",
    "        # Random feedback simulation: 70% chance of a click if product is popular\n",
    "        # Reward = 1 (click) or 0 (no click)\n",
    "        reward = 1 if random.random() < 0.7 else 0\n",
    "        next_state = (state + 1) % self.n_states  # Randomly moving to another state\n",
    "        self.update_q_table(state, action, reward, next_state)\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d822216-4233-4d0b-9345-3ba4de28d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Q-table:\n",
      "[[0.1 0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "Episode 100, Q-table:\n",
      "[[2.21274003 0.25487825 0.         0.26479937 0.38362905]\n",
      " [2.0881644  0.17753366 0.         0.         0.        ]\n",
      " [2.18431355 0.         0.         0.         0.        ]]\n",
      "Episode 200, Q-table:\n",
      "[[3.76788232 0.25487825 0.39784667 0.26479937 0.38362905]\n",
      " [3.6337533  0.17753366 0.         0.         0.32372187]\n",
      " [3.66231761 0.84645774 0.         0.32409548 0.        ]]\n",
      "Episode 300, Q-table:\n",
      "[[4.66473601 0.7169351  0.39784667 0.65363762 0.86058433]\n",
      " [4.57185109 0.58938888 0.         0.50359021 1.18605493]\n",
      " [4.65188001 1.26060499 0.         0.32409548 0.        ]]\n",
      "Episode 400, Q-table:\n",
      "[[5.22442336 1.19518572 0.80800613 0.65363762 0.86058433]\n",
      " [5.39722506 1.09811144 0.         0.9097289  1.55844151]\n",
      " [5.50230512 1.67877504 0.         0.32409548 0.        ]]\n",
      "Episode 500, Q-table:\n",
      "[[5.52923749 1.67910643 0.80800613 0.65363762 0.86058433]\n",
      " [5.64346574 1.09811144 1.12826973 0.9097289  1.99780482]\n",
      " [5.65472378 1.67877504 0.         0.88775334 0.50600912]]\n",
      "Episode 600, Q-table:\n",
      "[[5.79561695 2.03576809 1.33511743 1.09564785 0.86058433]\n",
      " [5.68468586 1.09811144 2.40253495 1.88099527 1.99780482]\n",
      " [5.7861602  1.67877504 1.63651449 1.29479323 0.50600912]]\n",
      "Episode 700, Q-table:\n",
      "[[6.22786219 2.37288901 1.8402951  1.09564785 1.41282501]\n",
      " [6.1909022  1.09811144 2.70143475 1.88099527 1.99780482]\n",
      " [6.13934947 1.67877504 1.63651449 2.17517683 1.10268293]]\n",
      "Episode 800, Q-table:\n",
      "[[6.3110432  2.37288901 1.8402951  1.09564785 1.41282501]\n",
      " [6.17541263 2.14099777 3.06675964 2.25276314 1.99780482]\n",
      " [6.30862095 2.63367486 2.14068635 3.02069806 1.10268293]]\n",
      "Episode 900, Q-table:\n",
      "[[6.54270963 2.37288901 1.8402951  1.09564785 1.84388376]\n",
      " [6.65378201 2.89581503 3.06675964 3.07933376 2.36146725]\n",
      " [6.66097553 2.63367486 3.01964924 3.02069806 1.67070478]]\n",
      "Final recommendation for user state 0: Product 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Define the environment and initial parameters\n",
    "n_products = 5  # Number of products\n",
    "n_states = 3  # Number of different user states (e.g., different user profiles)\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "# 2. Initialize the RL-based recommender system\n",
    "recommender = RLRecommender(n_products, n_states, learning_rate, discount_factor, epsilon)\n",
    "\n",
    "# 3. Simulate a few interactions\n",
    "for episode in range(1000):  # Simulate 1000 interactions\n",
    "    state = random.randint(0, n_states - 1)  # Random initial state (user profile)\n",
    "    action = recommender.get_recommendation(state)  # Get recommended product based on current state\n",
    "    reward = recommender.simulate_interaction(state, action)  # Simulate user feedback (click/no-click)\n",
    "    \n",
    "    if episode % 100 == 0:  # Print the Q-table every 100 episodes\n",
    "        print(f\"Episode {episode}, Q-table:\")\n",
    "        print(recommender.q_table)\n",
    "\n",
    "# 4. Make final recommendations after learning\n",
    "final_state = 0  # Assume the user is in state 0 (user profile)\n",
    "recommended_product = recommender.get_recommendation(final_state)\n",
    "print(f\"Final recommendation for user state {final_state}: Product {recommended_product}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719e786-c255-4e53-947f-9a0b0e22e94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4693d-0420-4bc3-a549-159d0b9d5ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
