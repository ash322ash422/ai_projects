{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Explanation of the Workflow:\n",
        "User Query: The bot receives a question from the user as input (query).\n",
        "\n",
        "Embed the Query: The query is converted into a vector embedding using OpenAI’s text-embedding-ada-002 \n",
        "model (high-dimensional representation of the text).\n",
        "\n",
        "Retrieve Relevant Documents: This query embedding is used to search the Pinecone vector database. Pinecone \n",
        "returns the top 5 most similar documents from its index based on the similarity between their embeddings\n",
        "and the query embedding.\n",
        "\n",
        "Generate Context: The retrieved documents’ content is concatenated into a context, which will help the GPT model\n",
        "generate an informed answer.\n",
        "\n",
        "Generate Final Answer: OpenAI GPT is then given both the original query and the context to produce a \n",
        "well-informed answer.\n",
        "\n",
        "Return the Answer: Finally, the answer generated by GPT is returned to the user.\n",
        "\n",
        "This pseudocode simplifies the entire RAG model into clear, sequential steps.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO5jfwmvulTA",
        "outputId": "ed7a9dd3-e625-4baa-8fa8-e465e6e7d7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone-client==5.0.1\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client==5.0.1)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client==5.0.1)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client, openai\n",
            "Successfully installed openai-0.28.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client==5.0.1 openai==0.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dvCpIcO1u6Mc"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "import openai\n",
        "\n",
        "OPENAI_API_KEY=\"your_key\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "PINECONE_API_KEY = \"your_key\"\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"medical-docs\") # NOTE: make sure this exist on the server. Dimension = 1536"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zjTIEDQeSWFL"
      },
      "outputs": [],
      "source": [
        "###################### Define Functions for Embedding and Document Retrieval\n",
        "# Create a function to generate embeddings using OpenAI's embedding model, and a\n",
        "# function to retrieve relevant documents using Pinecone.\n",
        "# Function to get embedding from OpenAI\n",
        "def get_embedding(text):\n",
        "    response = openai.Embedding.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=text\n",
        "    )\n",
        "    return response['data'][0]['embedding']\n",
        "###########################\n",
        "\n",
        "# Function to retrieve relevant documents from Pinecone\n",
        "def retrieve_documents(query, top_k=1):\n",
        "    query_embedding = get_embedding(query)\n",
        "\n",
        "    query_results = index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=1,\n",
        "    include_metadata=True  # Include metadata in the results to retrieve the text\n",
        "    )\n",
        "\n",
        "    documents = []\n",
        "    for match in query_results['matches']:\n",
        "        documents.append(match)\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IpH_OeeSrCD",
        "outputId": "4a9bfb0b-f7d8-47c6-ae25-db2b94c1843f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(embedding)= 1536\n",
            "len(embedding)= 1536\n",
            "Medical documents have been inserted into the index.\n"
          ]
        }
      ],
      "source": [
        "########################### Insert Documents into Pinecone Index\n",
        "# Here’s how to insert (upsert) documents into the Pinecone\n",
        "# index. Each document is converted into an embedding and stored with a unique ID.\n",
        "# Sample documents to insert\n",
        "documents = [\n",
        "    {\"id\": \"doc1\", \"text\": \"Medical document content for Doc 1. Side effects of Aspirin are headache, and ulcers\"},\n",
        "    {\"id\": \"doc2\", \"text\": \"Medical document content for Doc 2. Medications should be taken under supervision of doctors\"},\n",
        "]\n",
        "\n",
        "# Insert documents into the Pinecone index\n",
        "for doc in documents:\n",
        "    embedding = get_embedding(doc['text'])  # Get embedding from OpenAI model\n",
        "    print(\"len(embedding)=\", len(embedding))  # Should print 1536\n",
        "    index.upsert([{\n",
        "        'id': doc['id'],\n",
        "        'values': embedding,\n",
        "        'metadata': {'text': doc['text']}  # Include document text as metadata\n",
        "    }])\n",
        "\n",
        "print(\"Medical documents have been inserted into the index.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "80Ynd5NTSyCS"
      },
      "outputs": [],
      "source": [
        "#######################Generate Answer with GPT-4\n",
        "# Now, retrieve the most relevant documents from Pinecone based on a query\n",
        "# and generate an answer using  GPT-4.\n",
        "# Function to generate answer using OpenAI GPT\n",
        "def generate_answer(query, context):\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Function to get final answer from RAG model\n",
        "def rag_qa_bot(query):\n",
        "    # Retrieve relevant documents from Pinecone\n",
        "    retrieved_docs = retrieve_documents(query)\n",
        "    print(\"retrieved_docs=\",retrieved_docs)\n",
        "    print(\"len(retrieved_docs)=\",len(retrieved_docs))\n",
        "    # Prepare context by concatenating the retrieved document texts\n",
        "    context = \"\\n\".join([doc['metadata']['text'] for doc in retrieved_docs])\n",
        "    print(\"context=\",context)\n",
        "\n",
        "    # Generate the final answer using OpenAI GPT\n",
        "    final_answer = generate_answer(query, context)\n",
        "    return final_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWBdVBhDS2xq",
        "outputId": "34473c6e-d314-466c-8ef3-f2066e784019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "retrieved_docs= [{'id': 'doc1',\n",
            " 'metadata': {'text': 'Medical document content for Doc 1. Side effects of '\n",
            "                      'Aspirin are headache, and ulcers'},\n",
            " 'score': 0.898047745,\n",
            " 'values': []}]\n",
            "len(retrieved_docs)= 1\n",
            "context= Medical document content for Doc 1. Side effects of Aspirin are headache, and ulcers\n",
            "Answer: The side effects of taking Aspirin are headache, and ulcers.\n"
          ]
        }
      ],
      "source": [
        "#################################Test the QA Bot\n",
        "# Finally, test the RAG-based QA bot with a sample question.\n",
        "query = \"What are the side effects of taking Aspirin?\"\n",
        "answer = rag_qa_bot(query)\n",
        "\n",
        "print(\"Answer:\", answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
